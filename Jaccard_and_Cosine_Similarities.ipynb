{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc39f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from itertools import combinations   # for combinations\n",
    "\n",
    "import pandas as pd                   # to import\n",
    "import numpy as np                    # for unique()\n",
    "from nltk.stem import PorterStemmer   # for stemming   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da81463",
   "metadata": {},
   "source": [
    " ##### Creating cleaning function to clean words from any non-alphabetical signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e7b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(word):\n",
    "\n",
    "# here each word is being cleaned by any included non-alphabetical characters, \n",
    "# like in a case: car?, Holidays!, planets:, 'yummy', sweet, yucky. ...        \n",
    "# Besides that it excludes any non-alphabetical typos\n",
    "        \n",
    "    clean_word = ''        \n",
    "\n",
    "    for letter in word: \n",
    "# here i am removing any non-alphabetical characters from each word        \n",
    "        if letter.isalpha():\n",
    "            clean_word += letter\n",
    "        else:\n",
    "            next\n",
    "        \n",
    "    return clean_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f47d6",
   "metadata": {},
   "source": [
    "##### Importing stop words in english. We can also use built-in stop words in NLP package, but its magnitude is not as wide as those imported files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3441cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting all stopwords at hand together\n",
    "\n",
    "stopwords1 = pd.read_csv('stop-words_english_1_en.txt', header=None)\n",
    "stopwords2 = pd.read_csv('stop-words_english_2_en.txt', header=None)\n",
    "stopwords3 = pd.read_csv('stop-words_english_3_en.txt', header=None)\n",
    "stopwords4 = pd.read_csv('stop-words_english_4_google_en.txt', header=None)\n",
    "stopwords5 = pd.read_csv('stop-words_english_5_en.txt', header=None)\n",
    "stopwords6 = pd.read_csv('stop-words_english_6_en.txt', header=None)\n",
    "\n",
    "stopwords = pd.concat([stopwords1, stopwords2, stopwords3, stopwords4, stopwords5, stopwords6], axis=0)\n",
    "\n",
    "stopwords.index = [list(range(1, len(stopwords)+1))]\n",
    "stopwords.columns = ['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93244011",
   "metadata": {},
   "source": [
    "# Web Scrapping part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad74541",
   "metadata": {},
   "source": [
    "### **Set the URL you want to webscrape from**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0690c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://arxiv.org/abs/2007.08978', 'https://arxiv.org/abs/2006.03389', 'https://arxiv.org/abs/2006.16964',\n",
    "        'https://arxiv.org/abs/2007.04095', 'https://arxiv.org/abs/2008.00315', 'https://arxiv.org/abs/1901.08151',\n",
    "       'https://arxiv.org/abs/1901.10555', 'https://arxiv.org/abs/1511.03085', 'https://arxiv.org/abs/1905.03061',\n",
    "       'https://arxiv.org/abs/2002.01759']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4994dd1",
   "metadata": {},
   "source": [
    "##### Extracting the titles and the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081052fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "titles = []\n",
    "\n",
    "for each_link in urls:\n",
    "\n",
    "    # Connect to the URL\n",
    "    response = requests.get(each_link)\n",
    "    # Parse HTML and save to BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # to extract specific part: where the Abstract is located\n",
    "    abstract = soup.findAll('blockquote')\n",
    "    # compiling all abstracts into a list\n",
    "    abstracts += abstract\n",
    "\n",
    "    # to extract titles\n",
    "    title = soup.find('title')\n",
    "    titles += title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9655e",
   "metadata": {},
   "source": [
    "##### Let's us see the extracted titles to see if need further work to clean them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03001f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[2007.08978] A large-scale comparative analysis of Coding Standard conformance in Open-Source Data Science projects', '[2006.03389] Computability and Non-monotone induction', '[2006.16964] Data Science: Nature and Pitfalls', '[2007.04095] Data science and the art of modelling', '[2008.00315] A fresh look at introductory data science', '[1901.08151] Cloud BI: Future of Business Intelligence in the Cloud', '[1901.10555] The Effects of Using Business Intelligence Systems on an Excellence Management and Decision-Making Process by Start-Up Companies: A Case Study', '[1511.03085] Big Data and Business Intelligence: Debunking the Myths', '[1905.03061] Generalized formal model of big data', '[2002.01759] Quality Assurance Technologies of Big Data Applications: A Systematic Literature Review']\n"
     ]
    }
   ],
   "source": [
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19c1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_title in range(len(titles)):\n",
    "    # splitting all word junks and exluding the first chunk\n",
    "    texts = titles[each_title].split()[1:]\n",
    "    # rejoining the words of titles\n",
    "    titles[each_title] = ' '.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa718b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A large-scale comparative analysis of Coding Standard conformance in Open-Source Data Science projects', 'Computability and Non-monotone induction', 'Data Science: Nature and Pitfalls', 'Data science and the art of modelling', 'A fresh look at introductory data science', 'Cloud BI: Future of Business Intelligence in the Cloud', 'The Effects of Using Business Intelligence Systems on an Excellence Management and Decision-Making Process by Start-Up Companies: A Case Study', 'Big Data and Business Intelligence: Debunking the Myths', 'Generalized formal model of big data', 'Quality Assurance Technologies of Big Data Applications: A Systematic Literature Review']\n"
     ]
    }
   ],
   "source": [
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872a0b2",
   "metadata": {},
   "source": [
    "##### Now let's go on to the same procedures but with the extracted abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afee7baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  Background: Meeting the growing industry demand for Data Science requires\n",
      "cross-disciplinary teams that can translate machine learning research into\n",
      "production-ready code. Software engineering teams value adherence to coding\n",
      "standards as an indication of code readability, maintainability, and developer\n",
      "expertise. However, there are no large-scale empirical studies of coding\n",
      "standards focused specifically on Data Science projects. Aims: This study\n",
      "investigates the extent to which Data Science projects follow code standards.\n",
      "In particular, which standards are followed, which are ignored, and how does\n",
      "this differ to traditional software projects? Method: We compare a corpus of\n",
      "1048 Open-Source Data Science projects to a reference group of 1099 non-Data\n",
      "Science projects with a similar level of quality and maturity. Results: Data\n",
      "Science projects suffer from a significantly higher rate of functions that use\n",
      "an excessive numbers of parameters and local variables. Data Science projects\n",
      "also follow different variable naming conventions to non-Data Science projects.\n",
      "Conclusions: The differences indicate that Data Science codebases are distinct\n",
      "from traditional software codebases and do not follow traditional software\n",
      "engineering conventions. Our conjecture is that this may be because traditional\n",
      "software engineering conventions are inappropriate in the context of Data\n",
      "Science projects.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  Non-monotone inductive definitions were studied in the late 1960's and early\n",
      "1970's with the aim of understanding connections between the complexity of the\n",
      "formulas defining the induction steps and the size of the ordinals measuring\n",
      "the duration of the inductions. In general, any type 2 functional will generate\n",
      "an inductive process, and in this paper we will view non-monotone induction as\n",
      "a functional of type 3. We investigate the associated computation theory\n",
      "inherited from the Kleene schemes and we investigate the nature of the\n",
      "associated companion of sets with codes computable in non-monotone induction.\n",
      "The interest in this functional is motivated from observing that constructions\n",
      "via non-monotone induction appear as natural in classical analysis in its\n",
      "original form.\n",
      "<br/>There are two groups of results: We establish strong closure properties of\n",
      "the least ordinal without a code computable in non-monotone induction, and we\n",
      "provide a characterisation of the class of functionals of type 3 computable\n",
      "from non-monotone induction, a characterisation in terms of sequential\n",
      "operators working in transfinite time. We will also see that the full power of\n",
      "non-monotone induction is required when this principle is used to construct\n",
      "functionals witnessing the compactness of the Cantor space and of closed,\n",
      "bounded intervals.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  Data science is creating very exciting trends as well as significant\n",
      "controversy. A critical matter for the healthy development of data science in\n",
      "its early stages is to deeply understand the nature of data and data science,\n",
      "and to discuss the various pitfalls. These important issues motivate the\n",
      "discussions in this article.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  Datacentric enthusiasm is growing strong across a variety of domains. Whilst\n",
      "data science asks unquestionably exciting scientific questions, we argue that\n",
      "its contributions should not be extrapolated from the scientific context in\n",
      "which they originate. In particular we suggest that the simple-minded idea to\n",
      "the effect that data can be seen as a replacement for scientific modelling is\n",
      "not tenable. By recalling some well-known examples from dynamical systems we\n",
      "conclude that data science performs at its best when coupled with the subtle\n",
      "art of modelling.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  The proliferation of vast quantities of available datasets that are large and\n",
      "complex in nature has challenged universities to keep up with the demand for\n",
      "graduates trained in both the statistical and the computational set of skills\n",
      "required to effectively plan, acquire, manage, analyze, and communicate the\n",
      "findings of such data. To keep up with this demand, attracting students early\n",
      "on to data science as well as providing them a solid foray into the field\n",
      "becomes increasingly important. We present a case study of an introductory\n",
      "undergraduate course in data science that is designed to address these needs.\n",
      "Offered at Duke University, this course has no pre-requisites and serves a wide\n",
      "audience of aspiring statistics and data science majors as well as humanities,\n",
      "social sciences, and natural sciences students. We discuss the unique set of\n",
      "challenges posed by offering such a course and in light of these challenges, we\n",
      "present a detailed discussion into the pedagogical design elements, content,\n",
      "structure, computational infrastructure, and the assessment methodology of the\n",
      "course. We also offer a repository containing all teaching materials that are\n",
      "open-source, along with supplemental materials and the R code for reproducing\n",
      "the figures found in the paper.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  Cloud computing is gradually gaining popularity among businesses due to its\n",
      "distinct advantages over self-hosted IT infrastructures. Business Intelligence\n",
      "(BI) is a highly resource intensive system requiring large-scale parallel\n",
      "processing and significant storage capacities to host data warehouses. In\n",
      "self-hosted environments it was feared that BI will eventually face a resource\n",
      "crunch situation because it will not be feasible for companies to keep adding\n",
      "resources to host a neverending expansion of data warehouses and the online\n",
      "analytical processing (OLAP) demands on the underlying networking. Cloud\n",
      "computing has instigated a new hope for future prospects of BI. However, how\n",
      "will BI be implemented on cloud and how will the traffic and demand profile\n",
      "look like? This research attempts to answer these key questions in regards to\n",
      "taking BI to the cloud. The cloud hosting of BI has been demonstrated with the\n",
      "help of a simulation on OPNET comprising a cloud model with multiple OLAP\n",
      "application servers applying parallel query loads on an array of servers\n",
      "hosting relational databases. The simulation results have reflected that true\n",
      "and extensible parallel processing of database servers on the cloud can\n",
      "efficiently process OLAP application demands on cloud computing. Hence, the BI\n",
      "designer needs to plan for a highly partitioned database running on massively\n",
      "parallel database servers in which, each server hosts at least one partition of\n",
      "the underlying database serving the OLAP demands.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  The rapid increase in data volumes in companies has meant that momentous and\n",
      "comprehensive information gathering is barely possible by manual means.\n",
      "Business intelligence solutions can help here. They provide tools with\n",
      "appropriate technologies to assist with the collection, integration, storage,\n",
      "editing, and analysis of existing data. While almost only large companies were\n",
      "interested in this topic a few years ago, it has meanwhile also become\n",
      "necessary for start-up companies, and so the market for business intelligence\n",
      "has been growing for years. This article focuses on the general potentials of\n",
      "using BI in start-ups. First, will be examined which providers of BI solutions\n",
      "that are suitable for start-ups and what opportunities exist for implementing\n",
      "BI systems in start-ups. Then it will be shown to what extent BI has prevailed\n",
      "in start-ups, in which areas the techniques of BI are used in start-ups and\n",
      "what purpose BI has in start-ups. Finally, the success factors for BI projects\n",
      "in start-ups are considered.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  Big data is one of the most discussed, and possibly least understood, terms\n",
      "in use in business today. Big data is said to offer not only unprecedented\n",
      "levels of business intelligence concerning the habits of consumers and rivals,\n",
      "but also to herald a revolution in the way in which business are organized and\n",
      "run. However, big data is not as straightforward as it might seem, particularly\n",
      "when it comes to the so-called dark data from social media. It is not simply\n",
      "the quantity of data that has changed, it is also the speed and the variety of\n",
      "formats with which it is delivered. This article sets out to look at big data\n",
      "and debunk some of the myths that surround it. It focuses on the role of data\n",
      "from social media in particular and highlights two common myths about big data.\n",
      "The first is that because a data set contains billions of items, traditional\n",
      "methodological issues no longer matter. The second is the belief that big data\n",
      "is both a complete and unbiased source of data upon which to base decisions.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  This article dwells on the basic characteristic features of the Big Data\n",
      "technologies. It is analyzed the existing definition of the \"big data\" term.\n",
      "The article proposes and describes the elements of the generalized formal model\n",
      "of big data. It is analyzed the peculiarities of the application of the\n",
      "proposed model components. It described the fundamental differences between Big\n",
      "Data technology and business analytics. Big Data is supported by the\n",
      "distributed file system Google File System technology, Cassandra, HBase, Lustre\n",
      "and ZFS, by the MapReduce and Hadoop programming constructs and many other\n",
      "solutions. According to the experts, such as McKinsey Institute, the\n",
      "manufacturing, healthcare, trade, administration and control of individual\n",
      "movements undergo the transformations under the influence of the Big Data.\n",
      "\n",
      "    </blockquote>, <blockquote class=\"abstract mathjax\">\n",
      "<span class=\"descriptor\">Abstract:</span>  Big data applications are currently used in many application domains, ranging\n",
      "from statistical applications to prediction systems and smart cities. However,\n",
      "the quality of these applications is far from perfect, leading to a large\n",
      "amount of issues and problems. Consequently, assuring the overall quality for\n",
      "big data applications plays an increasingly important role. This paper aims at\n",
      "summarizing and assessing existing quality assurance (QA) technologies\n",
      "addressing quality issues in big data applications. We have conducted a\n",
      "systematic literature review (SLR) by searching major scientific databases,\n",
      "resulting in 83 primary and relevant studies on QA technologies for big data\n",
      "applications. The SLR results reveal the following main findings: 1) the impact\n",
      "of the big data attributes of volume, velocity, and variety on the quality of\n",
      "big data applications; 2) the quality attributes that determine the quality for\n",
      "big data applications include correctness, performance, availability,\n",
      "scalability, reliability and so on; 3) the existing QA technologies, including\n",
      "analysis, specification, model-driven architecture (MDA), verification, fault\n",
      "tolerance, testing, monitoring and fault &amp; failure prediction; 4) existing\n",
      "strengths and limitations of each kind of QA technology; 5) the existing\n",
      "empirical evidence of each QA technology. This study provides a solid\n",
      "foundation for research on QA technologies of big data applications. However,\n",
      "many challenges of big data applications regarding quality still remain.\n",
      "\n",
      "    </blockquote>]\n"
     ]
    }
   ],
   "source": [
    "print(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "958a8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in range(len(abstracts)):\n",
    "    # specifically extracting the Abstracts\n",
    "    abstracts[each] = abstracts[each].contents[2]\n",
    "    # stripping any spaces, tabs, or empty lines\n",
    "    abstracts[each] = abstracts[each].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29489d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Background: Meeting the growing industry demand for Data Science requires\\ncross-disciplinary teams that can translate machine learning research into\\nproduction-ready code. Software engineering teams value adherence to coding\\nstandards as an indication of code readability, maintainability, and developer\\nexpertise. However, there are no large-scale empirical studies of coding\\nstandards focused specifically on Data Science projects. Aims: This study\\ninvestigates the extent to which Data Science projects follow code standards.\\nIn particular, which standards are followed, which are ignored, and how does\\nthis differ to traditional software projects? Method: We compare a corpus of\\n1048 Open-Source Data Science projects to a reference group of 1099 non-Data\\nScience projects with a similar level of quality and maturity. Results: Data\\nScience projects suffer from a significantly higher rate of functions that use\\nan excessive numbers of parameters and local variables. Data Science projects\\nalso follow different variable naming conventions to non-Data Science projects.\\nConclusions: The differences indicate that Data Science codebases are distinct\\nfrom traditional software codebases and do not follow traditional software\\nengineering conventions. Our conjecture is that this may be because traditional\\nsoftware engineering conventions are inappropriate in the context of Data\\nScience projects.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb4351",
   "metadata": {},
   "source": [
    "##### Now we extract each word in abstracts that are not stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5558a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_abstracts = []\n",
    "\n",
    "for each_abstract in range(len(abstracts)):\n",
    "    \n",
    "    temp_list = []\n",
    "    \n",
    "    for each_word in abstracts[each_abstract].split():\n",
    "        # cleaning words from any non-alphabetical signs\n",
    "        clean_word = cleaning(each_word).lower()\n",
    "        \n",
    "        # excluding words if they are in stopwords\n",
    "        if clean_word in list(stopwords['word']):\n",
    "            next\n",
    "        elif clean_word == '':\n",
    "            next\n",
    "        # collecting filtered out new words\n",
    "        else:\n",
    "            # doing stemming on each word\n",
    "            temp_list.append(clean_word)\n",
    "\n",
    "    # excluding out duplicate values\n",
    "    temp_list = list(np.unique(temp_list))\n",
    "    \n",
    "    # re-compile all abstracts under a new list, new_abstract\n",
    "    new_abstracts.append(temp_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74a35aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adherence aims background code codebases coding compare conclusions conjecture context conventions corpus crossdisciplinary data demand developer differ differences distinct empirical engineering excessive expertise extent focused follow functions group growing higher inappropriate indication industry investigates largescale learning level local machine maintainability maturity meeting method naming nondata numbers opensource parameters productionready projects quality rate readability reference requires science software standards studies study suffer teams traditional translate variable variables'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(new_abstracts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9d376",
   "metadata": {},
   "source": [
    "##### Now we can run on Jaccard similarities and Cosine similarities on the cleaned word of each abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe89e0",
   "metadata": {},
   "source": [
    "# Finding Similarities part & Recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcdb414",
   "metadata": {},
   "source": [
    "## Jaccard Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0844e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jaccard\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70cea3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard_Similarity(doc1, doc2): \n",
    "    \n",
    "    # List the unique words in a document\n",
    "    words_doc1 = set(doc1) \n",
    "    words_doc2 = set(doc2)\n",
    "    \n",
    "    # Find the intersection of words list of doc1 & doc2\n",
    "    intersection = words_doc1.intersection(words_doc2)\n",
    "\n",
    "    # Find the union of words list of doc1 & doc2\n",
    "    union = words_doc1.union(words_doc2)\n",
    "        \n",
    "    # Calculate Jaccard similarity score \n",
    "    # using length of intersection set divided by length of union set\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f5ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating all possibilities of getting pairs on Papers to run\n",
    "\n",
    "possible_pairs = list(combinations(range(len(new_abstracts)), 2))\n",
    "#possible_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1227dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating similarities\n",
    "\n",
    "similarities = []\n",
    "\n",
    "for each in possible_pairs:\n",
    "    similarities.append(list([each[0], each[1], \n",
    "                              Jaccard_Similarity(new_abstracts[each[0]], new_abstracts[each[1]])]))\n",
    "    \n",
    "# similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "769d5d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item1</th>\n",
       "      <th>Item2</th>\n",
       "      <th>Similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.059603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.059406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.057971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.052083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Item1  Item2  Similarities\n",
       "1      4      9      0.059603\n",
       "2      6      8      0.059406\n",
       "3      2      3      0.058824\n",
       "4      2      7      0.057971\n",
       "5      2      4      0.052083"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_table = pd.DataFrame(similarities, \n",
    "                                columns = ['Item1', 'Item2', 'Similarities']).sort_values('Similarities', ascending=False)\n",
    "similarity_table.index = [list(range(1, len(similarities)+1))] \n",
    "\n",
    "similarity_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383cea6f",
   "metadata": {},
   "source": [
    "##### So, according to the Jaccard similarity algorithm the above table shows similar papers based on the words in their abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca083b",
   "metadata": {},
   "source": [
    "## Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8108c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "\n",
    "    l1 =[]\n",
    "    l2 =[]\n",
    "\n",
    "    X_set = set(x)\n",
    "    Y_set = set(y)\n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector:\n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0)\n",
    "        if w in Y_set: l2.append(1)\n",
    "        else: l2.append(0)\n",
    "    c = 0\n",
    "  \n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)):\n",
    "        c+= l1[i]*l2[i]\n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0253cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating similarities\n",
    "\n",
    "similarities = []\n",
    "\n",
    "for each in possible_pairs:\n",
    "    similarities.append(list([each[0], each[1], cosine_similarity(new_abstracts[each[0]], new_abstracts[each[1]])]))\n",
    "    \n",
    "# similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56942bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item1</th>\n",
       "      <th>Item2</th>\n",
       "      <th>Similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.121988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.121046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.112154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Item1  Item2  Similarities\n",
       "1      2      4      0.121988\n",
       "2      2      7      0.121046\n",
       "3      2      3      0.113961\n",
       "4      4      9      0.112500\n",
       "5      6      8      0.112154"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_table = pd.DataFrame(similarities, \n",
    "                                columns = ['Item1', 'Item2', 'Similarities']).sort_values('Similarities', ascending=False)\n",
    "similarity_table.index = [list(range(1, len(similarities)+1))] \n",
    "\n",
    "similarity_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef6509",
   "metadata": {},
   "source": [
    "##### The above code finds all similar papers based on the abstracts of each paper using Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505bc17",
   "metadata": {},
   "source": [
    "##### If we pay attention to the both above tables, even though the calculated values for similarities differ from one algorithm to another, the overall output is somehow similar. Both algorithms arranged similar papers differently, but the first 5 output are in general the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc66ec6",
   "metadata": {},
   "source": [
    "One can choose the algorithm based on the given task and preferences.\n",
    "\n",
    "**'Jaccard similarity is good for cases where duplication does not matter, cosine similarity is good for cases where duplication matters while analyzing text similarity. For two product descriptions, it will be better to use Jaccard similarity as repetition of a word does not reduce their similarity'.** Source: https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
